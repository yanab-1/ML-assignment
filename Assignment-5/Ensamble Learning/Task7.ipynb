{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efba3653",
   "metadata": {},
   "source": [
    "Task 7: Conceptual Questions Answer:\n",
    "\n",
    "1. What is the difference between Bagging and Boosting?\n",
    "2. How does Random Forest reduce variance?\n",
    "3. What is the weakness of boosting-based methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822ca531",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4255113323.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mBagging (Bootstrap Aggregating):-\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "1. What is the difference between Bagging and Boosting?\n",
    "\n",
    "Bagging (Bootstrap Aggregating):-\n",
    "Builds multiple models independently using random subsets of the training data (with replacement).\n",
    "Averages their predictions (or uses majority voting) to improve stability and reduce variance.\n",
    "Examples: Random Forest, Bagged Decision Trees.\n",
    "\n",
    "Boosting:-\n",
    "Builds models sequentially, where each new model focuses on correcting errors made by the previous ones.\n",
    "Combines them to form a strong learner.\n",
    "Reduces both bias and variance, but more prone to overfitting.\n",
    "Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "2. How does Random Forest reduce variance?\n",
    "\n",
    "Random Forest builds multiple decision trees using different random subsets of:-\n",
    "Training data (via bootstrapping).\n",
    "Features (random feature selection at each split).\n",
    "By averaging predictions from many uncorrelated trees, it:\n",
    "Smooths out noise from individual trees.\n",
    "Reduces overfitting.\n",
    "\n",
    "Results in lower variance and more robust predictions compared to a single decision tree.\n",
    "\n",
    "3. What is the weakness of boosting-based methods?\n",
    "\n",
    "Sensitive to Noisy Data and Outliers:-\n",
    "Because boosting tries to correct the errors of previous models, it can over-focus on noisy or mislabelled data, leading to overfitting.\n",
    "\n",
    "Longer Training Time:-\n",
    "Boosting is sequential, so it can be slower to train compared to parallel methods like bagging.\n",
    "\n",
    "Complexity:\n",
    "Boosting models are more complex and harder to interpret than simpler models like decision trees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
